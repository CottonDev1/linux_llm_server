[Unit]
Description=LLaMA General Model Server (Qwen2.5-3B on GPU 1)
After=network.target

[Service]
Type=simple
User=chad
Group=chad
Environment="CUDA_VISIBLE_DEVICES=1"
ExecStart=/data/projects/llama.cpp/build/bin/llama-server \
    -m /data/models/qwen2.5-3b-instruct-q4_k_m.gguf \
    --host 0.0.0.0 \
    --port 8081 \
    --n-gpu-layers 99 \
    --metrics --ctx-size 4096
Restart=on-failure
RestartSec=10
StandardOutput=append:/data/logs/llama/general.log
StandardError=append:/data/logs/llama/general.log

[Install]
WantedBy=multi-user.target
